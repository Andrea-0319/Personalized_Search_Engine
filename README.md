# Personalized QA Retrieval & Recommendation System ğŸ¤–ğŸ“š

## Aim of the project ğŸ¯  
Build and evaluate a personalized retrieval pipeline over a large-scale community Q&A corpus (SE-PQA), demonstrating how lexical, semantic and user-specific signals can be combined to surface more relevant answers for each user.

## Description ğŸ“  
This project uses the SE-PQA dataset (~1.1 M questions, 2.17 M answers, 589 k users across 50 StackExchange sites) to design and implement an end-to-end information retrieval and personalization workflow. From indexing with BM25 through neural re-ranking, query expansion, and tag-based personalization, we explore how each component contributes to ranking quality and user satisfaction.  

**Pipeline Overview:**  
1. **Data Preparation & Indexing** ğŸ”§  
   - Clean and normalize text (case-folding, lemmatization)  
   - Build a unified PyTerrier index with Elasticsearch (BM25) first stage  
2. **Baseline & Parameter Tuning** âš™ï¸  
   - Grid search on BM25 parameters (kâ‚, b) to optimize Recall@100, P@1, MAP, NDCG  
3. **Neural Re-ranking** ğŸ§   
   - Embed and re-rank with MiniLM (cosine similarity)  
   - Fine-tune DistilBERT QA & MonoT5 for answer selection  
4. **Query Expansion** âœï¸  
   - Generate expanded queries using T5-small (controlled max_new_tokens)  
5. **Personalization** ğŸ·ï¸  
   - Compute tag-overlap scores between asker and answerer (â€œTAGâ€ model)  
6. **Fusion Strategies** ğŸ”„  
   - Combine BM25, neural scores, expansion and TAG via Reciprocal Rank Fusion (RRF) or weighted sum  
7. **Evaluation** ğŸ“Š  
   - Compare metrics across base vs. personalized setups: P@1, Recall@100, MAP@100, NDCG@3  

## Objectives ğŸ¥…  
- Establish a strong BM25 baseline and tune its hyperparameters  
- Assess the impact of neural re-ranking models (MiniLM, DistilBERT QA, MonoT5)  
- Quantify gains from T5-based query expansion  
- Integrate lightweight tag-based personalization for user-aware ranking  
- Devise effective fusion strategies to blend all signals  

## Main Results ğŸ†  
- **BM25 Baseline:**  
  - P@1 â‰ˆ 0.71 | Recall@100 â‰ˆ 0.93 | MAP@100 â‰ˆ 0.77 | NDCG@3 â‰ˆ 0.77  
- **Neural Re-ranking:**  
  - MonoT5-base fine-tuning yields +47 % relative MAP@100 improvement  
- **Query Expansion (T5):**  
  - Optimal `max_new_tokens=10` delivers modest P@1 uplift (~0.72)  
- **Fusion:**  
  - Weighted sum (BM25 0.2, neural 0.15 each, T5 0.5) boosts NDCG@3 to ~0.78  
- **Personalization (TAG):**  
  - Adds up to +8 % relative MAP@100 and consistent gains in P@1 & NDCG when fused with any model  

## Technologies Used ğŸ’»  
- **Python**  
- **PyTerrier** (retrieval pipeline)  
- **Elasticsearch / BM25** (first-stage indexing)  
- **HuggingFace Transformers** (MiniLM, DistilBERT QA, MonoT5, T5-small)  
- **Adapters** (parameter-efficient fine-tuning)  
- **rangx** (evaluation metrics)  
- **Jupyter Notebook** (experimentation & reporting)  
- Utility libraries: **pandas**, **NumPy**, **scikit-learn**, **json**, **pickle**
